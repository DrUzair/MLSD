---
title: "Understanding Gradient Descent Algorithm"
author: "Uzair Ahmad"
date: "January 25, 2018"
output: html_document
runtime: shiny
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=TRUE)
```
# {.tabset}

## Intrdoduction 
This tutorial walks you through the steps of how to implement logistic regression with gradient descent algorithm.
We are going to use a synthetic data sets
```{r, echo=TRUE}
library(lattice)
flower_data <- data.frame(read.table('./data/flower2.csv', header = FALSE, sep = ",", col.names = c('x1', 'x2', 'c')))
xyplot(x1 ~ x2, data = flower_data, groups = c, pch = 19, cex = 1.5, main='Actual')

ui <- fluidPage(
  sidebarLayout(
    sidebarPanel(
        selectInput("dataset_name", "Dataset:",
              c("Flower" = "flower2.csv", "Eyeglass" = "eyeglass.csv", "Eyeball" = "eyeball.csv"),
              selected = 1),
    sliderInput("learning_rate", "Learning Rate:", min = 1, max = 2, value = 1.2)
    ),
    mainPanel(
      tabsetPanel(
        tabPanel("Dataset", plotOutput("distPlot"))
        #, 
        #tabPanel("Model Predictions", plotOutput("distPlot")), 
        #tabPanel("Cost Function", plotOutput("distPlot"))
      )
    )
  )
)

server <- function(input, output){
  output$distPlot <- renderPlot({
    dataset_name <- paste0("./data/", input$dataset_name)
    print(dataset_name)
    flower_data <- data.frame(read.table
                                (dataset_name, 
                                         header = FALSE, 
                                         sep = ",", 
                                         col.names = c('x1', 'x2', 'c')
                                )
                              )
    X_train <- as.matrix(flower_data[c('x1', 'x2')])
    X_train <- scale(X_train)
    X_train <- t(X_train)
    Y_train <- t(as.matrix(flower_data[c('c')]))
    xyplot(x1 ~ x2, data = flower_data, groups = c, pch = 19, cex = 1.5, main='Actual')
  })
}
shinyApp(ui = ui, server = server)
```


## Analytical Approach {.tabset .tabset-fade .tabset-pills}
Probability $p =   \frac{n}{N} 0 \ge p \le 1$     

Odds          $odds(p) =   \frac{p}{(1 - p)}                 =   e^{(W^Tx  + b)}$  

$log(odds)     =   logit(p)                    =   log({p}/{(1-p)})   =   W^Tx  + b  \Rightarrow  [-inf \ge log(odds) \le inf]$  

$e^{(log(odds))}=   e^{log(\frac{p}{(1-p)})}            =   odds                  =   e^{(W^Tx  + b)}$    

probability $p  = \frac{\frac{p}{1-p}}{1 + \frac{p}{1-p}} = \frac{odds(p)}{1+odds(p)} =  \frac{e^{(W^Tx  + b)}}{(1+e^{(W^Tx  + b)})} =   \frac{1}{(1+e^{(-(W^Tx+b)}))} \rightarrow \begin{cases} 1 & \text{if } & mx+b & \text{a large -ve},\\ 0 & \text{if } & mx+b & \text{a large +ve} \end{cases}$  

$oddsratio     =   \frac{odds(1)}{odds(2)}$
b1            =   log(odds1) - log(odds2)     =   log(odds1/odds2)      =   log(oddsratio)    --> b1 is logistic regression coef  
exp(b1)       =   exp(log(oddsratio))         =   oddsratio          

Logistic regression is in reality an ordinary regression using the logit as the response variable.
The logit transformation allows for a linear relationship between the response variable and the coefficients:

### Applying Logistic Regression
Using glm function, create a logistic regression model that can classify the color of flower petals.
```{r, echo=TRUE, dependson = "flower_data"}
mdl <- glm(c~., data=flower_data, family=binomial )
```

### Predict the response
```{r, echo=TRUE}
glm_pred   <- (predict(mdl, data= flower_data[c('x1', 'x2')], type = c("response") ) > 0.5) * 1
flower_data <- cbind(flower_data, glm_pred = glm_pred)
head(flower_data)
```

### Visualizing  Decision Boundary of the Logistic Regression Classifier
```{r, echo=TRUE}
intercept <- coef(mdl)[2]/(-coef(mdl)[3])
slope <- coef(mdl)[1]/(-coef(mdl)[3]) 
abs_error <- abs(flower_data$c - flower_data$glm_pred)
print(abs_error)
glm_mae <- round(mean(abs_error) * 100, 4)

xyplot(x1 ~ x2, data = flower_data, groups = glm_pred, pch = 19, cex = 1.5, main=paste0('Predictions MAE : ', glm_mae) ,
       #auto.key=list(x=0.05,y=0.95,text=c("Correct","Wrong")),
       panel = function( ...) {
         panel.xyplot(...)
         panel.abline(intercept , slope)
         panel.grid(...)
       }
)
```





## Gradient Descent Approach {.tabset .tabset-fade .tabset-pills}
### Sigmoid Activation
```{r, echo=TRUE}
sigmoid <- function (z){  return( 1 / ( 1 + exp(-z))) }
```
### Calculate Gradients
```{r calc_grads, echo=TRUE}
calc_grads <- function(w, b, X, Y){
  m = dim(X)[2]
  
  z = t(w) %*% X + b
  A = sigmoid(z)
  c = - 1/m * sum((Y * log(A)) + ((1 - Y) * log (1-A)))
  A_minus_Y = A - Y
  dw = as.matrix(1/m * (X %*% t(A_minus_Y)))
  db = 1/m * (sum(A_minus_Y))
  g = list(dw = dw, db = db)
  return (list(grads = g, cost = c))
}
```
### Visualize Loss Function
```{r, echo=TRUE}
sigmoid <- function (z){  return( 1 / ( 1 + exp(-z))) }calc_grads <- function(w, b, X, Y){  m = dim(X)[2]    z = t(w) %*% X + b  A = sigmoid(z)  c = - 1/m * sum((Y * log(A)) + ((1 - Y) * log (1-A)))  A_minus_Y = A - Y  dw = as.matrix(1/m * (X %*% t(A_minus_Y)))  db = 1/m * (sum(A_minus_Y))  g = list(dw = dw, db = db)  return (list(grads = g, cost = c))}
setwd('C:\\RnD\\Dev\\gradient_descent')flower_data <- data.frame(read.table('flower2.csv', header = FALSE, sep = ",", col.names = c('x1', 'x2', 'c')))xyplot(x1 ~ x2, data = flower_data, groups = c, pch = 19, cex = 1.5, main='Actual')X_train <- as.matrix(flower_data[c('x1', 'x2')])X_train <- scale(X_train)X_train <- t(X_train)Y_train <- t(as.matrix(flower_data[c('c')]))

library(rgl)w_b_cost <- setNames(data.frame(matrix(ncol = 3, nrow = 0)), c("w", "b", "cost"))w_list <- seq(from = -1, to = 1, by = 0.1)b_list <- seq(from = -1, to = 1, by = .1)for(i in w_list){  w   = as.matrix(rep(i, dim(X_train)[1]), nrow = dim(X_train)[1], ncol = 1)  for(j in b_list){    b   = j    cost <- calc_grads(w, b, X_train, Y_train)$cost    print(cost)    w_b_cost <- rbind(w_b_cost, data.frame('w'= w, 'b'= b, 'cost'= cost))  }}plot3d(w_b_cost$w, w_b_cost$b, w_b_cost$cost,        main = 'Cost Function', xlab='Weight', ylab='Bias' ,zlab='Cost',       type = "p", col = rainbow(7))

```
### Parameter Optimization
```{r, echo=TRUE}
optimize <- function(w, b, X, Y, num_iterations, alpha, print_cost = False){
  cost_list = list()
  for (i in seq(1, num_iterations)){
    ff_output = calc_grads(w, b, X, Y)
    
    dw = ff_output$grads$dw
    db = ff_output$grads$db
    
    w = as.matrix(w - alpha * dw)
    b = b - alpha * db
    
    cost_list[i] <- ff_output$cost
    
    print(paste0('cost after epoch ', i, ' is ',ff_output$cost))
    
  }
  params = list(w = w, b = b)
  grads = list(dw = dw, db = db)
  return (list(params=params, grads=grads, cost=cost_list))
}
```
### Predict Function
```{r, echo=TRUE}
predict.gd <- function(w, b, X){
  m = dim(X)[2]
  A = sigmoid(t(w) %*% X + b)
  Y_pred <- (A > 0.5) * 1
  return (Y_pred)
}
```


### GDLR Model Wrapper Function
```{r, echo=TRUE}
gdlr_model <- function(X_train, Y_train, X_test, Y_test, num_iterations = 2000, alpha = 0.5, print_cost = False){
  w   = as.matrix(rep(0, dim(X_train)[1]), nrow = dim(X_train)[1], ncol = 1)
  b   = 0
  
  mdl = optimize(w, b, X_train, Y_train, num_iterations, alpha, print_cost)
  
  w = mdl$params$w
  b = mdl$params$b
  
  Y_pred_test <- predict.gd(w, b, X_test)
  Y_pred_train <- predict.gd(w, b, X_train)
  
  print(paste0("Train MAE ", mean(abs(Y_pred_train - Y_train)) * 100))
  print(paste0("Test MAE ", mean(abs(Y_pred_test - Y_train)) * 100))
  
  return(mdl)
  
}
```

### Put to test
```{r, echo=TRUE}
X_train <- as.matrix(flower_data[c('x1', 'x2')])
X_train <- scale(X_train)
X_train <- t(X_train)
Y_train <- t(as.matrix(flower_data[c('c')]))

X_test <- X_train
Y_test <- Y_train

print(dim(Y_train))
gdlr_mdl <- gdlr_model (X_train, Y_train, X_test, Y_test, num_iterations = 100, alpha = 0.1, print_cost = False)
```
### GDLR Prediction Results
```{r, echo=TRUE}
gdlr_pred <- predict.gd(gdlr_mdl$params$w, gdlr_mdl$params$b, X_train)
gdlr_pred <- t(gdlr_pred)
flower_data <- cbind(flower_data, gdlr_pred = gdlr_pred)
abs_error_gdlr <- abs(flower_data$c - flower_data$gdlr_pred)
gdlr_mae <- round(mean(abs_error_gdlr) * 100, 4)
xyplot(x1 ~ x2, data = flower_data, groups = gdlr_pred, pch = 19, cex = 1.5, main=paste0('Predictions MAE : ', gdlr_mae))
```


## Gradient Descent with One Hidden Layer {.tabset .tabset-fade .tabset-pills}
### Parameter Initialization
Zero weights and symmetry:
Random weights and breaking symmetry:
Xavier initialization $W^{[l]*\sqrt\frac{1}{l-1}}$
[He Initialization](http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7410480): $W^{[l]*\sqrt\frac{2}{l-1}}$

```{r, echo=TRUE}
init_param <- function(nX, nH, nY){
  W1 <- matrix(runif(nX * nH, min = -1, max = 1) * 0.01, nrow = nH, ncol = nX)
  b1 <- c(rep(0, nH))
  W2 <- matrix(runif(nY * nH, min = -1, max = 1) * 0.01, nrow = nY, ncol = nH)
  b2 <- c(rep(0, nY))
  
  param <- list( W1 = W1,
                 b1 = b1,
                 W2 = W2,
                 b2 = b2)
  return(param)
}
```
### Forward Propagation
```{r, echo=TRUE}
forward_pass <- function(X, param){
  W1 <- param$W1
  b1 <- param$b1
  W2 <- param$W2
  b2 <- param$b2
  
  Z1 <- W1 %*% X
  Z1 <- Z1 + b1
  A1 <- tanh(Z1)
  Z2 <- W2 %*% A1 + b2
  A2 <- sigmoid(Z2)
  
  output <- list(Z1 = Z1,
                 A1 = A1,
                 Z2 = Z2,
                 A2 = A2)
  
  return (output)
} 
```
### Cost Function
```{r, echo=TRUE}
compute_cost <- function(A2, Y, param){
  m = length(Y)
  logprobs <- (log(A2) * Y) + (log(1 - A2) * (1 -Y))
  cost <- (- 1/m) * sum(logprobs)
  return (cost)
}

```
### Backward Propagation
```{r, echo=TRUE}
backward_pass <- function(param, cache, X, Y){
  m <- dim(X)[2] # number of examples in X
  
  W1 <- param$W1
  W2 <- param$W2
  A1 <- cache$A1
  A2 <- cache$A2
  
  dZ2 <- A2 - Y
  
  dW2 <- (1/m) * (dZ2 %*% t(A1)) 
  db2 <- matrix((1/m) * rowSums(dZ2))
  
  dZ1 <- (t(W2) %*% dZ2) * (1 - A1^2)
  
  dW1 <- (1/m) * (dZ1 %*% t(X))
  db1 <- matrix((1/m) * (rowSums(dZ1)))
  
  grads <- list( dW1 = dW1,
                 db1 = db1,
                 dW2 = dW2,
                 db2 = db2)
  
  return (grads)
}
```
### Tuning Parameters
```{r, echo=TRUE}
update_param <- function(param, grads, learning_rate = 1.2){
  W1 <- param$W1
  b1 <- param$b1
  W2 <- param$W2
  b2 <- param$b2
  
  dW1 <- grads$dW1
  db1 <- grads$db1
  dW2 <- grads$dW2
  db2 <- grads$db2
  
  W1 <- W1 - (learning_rate * dW1)
  b1 <- as.vector(b1 - (learning_rate * db1))
  W2 <- W2 - (learning_rate * dW2)
  b2 <- as.vector(b2 - (learning_rate * db2))
  
  param <- list(W1 = W1,
                b1 = b1,
                W2 = W2,
                b2 = b2)
  
  return(param)
}
```
### Model Wrapper
```{r, echo=TRUE}
mlp_train <- function(X, Y, n_h, num_iter = 1000, learning_rate = 1.2, print_cost=FALSE){
  n_x <- dim(X)[1]
  n_y <- dim(Y)[1]
  
  
  param <- init_param(n_x, n_h, n_y)
  
  W1 <- param$W1
  b1 <- param$b1
  W2 <- param$W2
  b2 <- param$b2
  
  costs = list()
  for (i in seq(1, num_iter)){
    
    fp_output = forward_pass(X, param)
    
    cost <- compute_cost(fp_output$A2, Y, param)
    
    costs[i] <- cost
    
    grads <- backward_pass(param, fp_output, X, Y)
    
    param <- update_param(param, grads, learning_rate = 1.2)
    
    if( i %% 50 == 0)
      print(paste0('cost after iteration ', i, ' is ' , cost))
  }
  return(list(param = param, costs = costs))
}
```
### Tuning Hyperparameters
Gradient Descent algorithm will only optimize the network parameters (i.e. weights and biases) within a given training scenario. It cannot learn optimal choices for the **hyperparameters** that define the training scenario. 
There are several hyperparamters that a network designer must consider i.e. number of hidden layers, neurons per layer, activation functions, number of iterations and learning rate.
Try this interactive neural network interface to experiment and learn the effect of hyperparameters.
Depending on your choices in the left panel, you can see the resulting predictions and cost function outputs in the main panel.
```{r, echo=FALSE}
ui <- fluidPage(
  sidebarLayout(
    sidebarPanel(
        selectInput("dataset_name", "Dataset:",
                    c("Flower" = "flower2.csv", "Eyeglass" = "eyeglass.csv", "Eyeball" = "eyeball.csv"),
                    selected = 1),
        sliderInput("learning_rate", "Learning Rate:", min = 1, max = 2, value = 1.2),
        sliderInput("num_hidden", "Hidden Layer Neurons:", min = 1, max = 10, value = 2),
        sliderInput("num_iter", "Iterations:", min = 100, max = 5000, value = 1000)
    ),
    mainPanel(
      tabsetPanel(
        tabPanel("Dataset", plotOutput("dataPlot")),
        tabPanel("Model Predictions", plotOutput("predplot")),
        tabPanel("Cost Function", plotOutput("costplot"))
      )
    )
  )
)

server <- function(input, output){
  
  
  print('hello world')
  output$dataPlot <- renderPlot({
    dataset_name <- paste0("./data/", input$dataset_name)
    print(dataset_name)
    flower_data <- data.frame(read.table
                                (dataset_name, 
                                         header = FALSE, 
                                         sep = ",", 
                                         col.names = c('x1', 'x2', 'c')
                                )
                              )
    
    
  
    xyplot(x1 ~ x2, data = flower_data, groups = c, pch = 19, cex = 1.5, main='Actual')
  })
  output$predplot <- renderPlot({
    
    learning_rate <- input$learning_rate
    nH <- input$num_hidden
    num_iter <- input$num_iter
    
    dataset_name <- paste0("./data/", input$dataset_name)
    print(dataset_name)
    flower_data <- data.frame(read.table
                                (dataset_name, 
                                         header = FALSE, 
                                         sep = ",", 
                                         col.names = c('x1', 'x2', 'c')
                                )
                              )
    X_train <- as.matrix(flower_data[c('x1', 'x2')])
    X_train <- scale(X_train)
    X_train <- t(X_train)
    Y_train <- t(as.matrix(flower_data[c('c')]))
  
    train_output <- mlp_train (X_train, Y_train, nH, num_iter, learning_rate)
    
    param <- train_output$param
    
    nl_pred <- (forward_pass(X_train, param)$A2 > 0.5) * 1
    nl_pred <- t(nl_pred)
    abs_error_nlpred <- abs(flower_data$c - nl_pred)
    nl_mae <- round(mean(abs_error_nlpred) * 100, 4)
  
    xyplot(x1 ~ x2, 
           data = flower_data, 
           groups = nl_pred, 
           pch = 19, 
           cex = 1.5, 
           main=paste0('Predictions MAE: ', nl_mae))
  })
  output$costplot <- renderPlot({
    
    learning_rate <- input$learning_rate
    nH <- input$num_hidden
    num_iter <- input$num_iter
  
    
    dataset_name <- paste0("./data/", input$dataset_name)
    print(dataset_name)
    flower_data <- data.frame(read.table
                                (dataset_name, 
                                         header = FALSE, 
                                         sep = ",", 
                                         col.names = c('x1', 'x2', 'c')
                                )
                              )
    
    X_train <- as.matrix(flower_data[c('x1', 'x2')])
    X_train <- scale(X_train)
    X_train <- t(X_train)
    Y_train <- t(as.matrix(flower_data[c('c')]))
    
    train_output <- mlp_train (X_train, Y_train, nH, num_iter, learning_rate)
    
    costs <- train_output$costs
    plot(x = seq(1, length(costs)) , 
         y = costs, 
         ylab = 'cost ', 
         xlab = 'iteration #', 
         main = paste0('learning rate ', learning_rate,
                       ', iterations ', num_iter) )
    })
}

shinyApp(ui = ui, server = server)
```

